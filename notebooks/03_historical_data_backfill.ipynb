{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Sentiment Trends Analysis - Historical Data Backfill\n",
        "## Phase 1C: Building Production Dataset for Backtesting\n",
        "\n",
        "This notebook implements large-scale historical data collection and processing to create a comprehensive dataset for trading strategy development and backtesting.\n",
        "\n",
        "### Phase 1C Objectives\n",
        "1. **Historical Market Data**: 1+ year of OHLCV data for all symbols\n",
        "2. **Technical Indicators**: Complete SMA/ROC calculations for full dataset\n",
        "3. **Multi-Symbol Processing**: INTC, AMD, NVDA with comparative analysis\n",
        "4. **Performance Optimization**: Efficient batch processing with progress monitoring\n",
        "5. **Data Validation**: Comprehensive integrity checks and quality assurance\n",
        "\n",
        "### Target Dataset\n",
        "- **Primary Symbol**: Intel Corporation (INTC)\n",
        "- **Competitors**: AMD, NVIDIA (NVDA)\n",
        "- **Time Period**: 1 year (252+ trading days)\n",
        "- **Data Points**: ~750 trading days across 3 symbols = 2,250+ records\n",
        "\n",
        "üöÄ **Ready to execute Phase 1C - Historical Data Backfill!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Environment loaded and libraries imported successfully!\n",
            "üîÑ Phase 1C: Historical Data Backfill Ready!\n",
            "üìä Target symbols: ['INTC', 'AMD', 'NVDA']\n",
            "‚öôÔ∏è  Batch size: 50\n",
            "‚è±Ô∏è  Rate limit delay: 0.1s\n"
          ]
        }
      ],
      "source": [
        "# Add parent directory to Python path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path('../').resolve()))\n",
        "\n",
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta, date\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Progress monitoring\n",
        "from tqdm.auto import tqdm\n",
        "import logging\n",
        "\n",
        "# Financial APIs\n",
        "import yfinance as yf\n",
        "\n",
        "# Database and utilities\n",
        "import sqlalchemy\n",
        "from dotenv import load_dotenv\n",
        "from src.database import get_database_connection, get_api_key\n",
        "\n",
        "# Load environment\n",
        "load_dotenv()\n",
        "\n",
        "# Configure logging for batch processing\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"‚úÖ Environment loaded and libraries imported successfully!\")\n",
        "print(\"üîÑ Phase 1C: Historical Data Backfill Ready!\")\n",
        "\n",
        "# Configuration\n",
        "TARGET_SYMBOLS = ['INTC', 'AMD', 'NVDA']\n",
        "BATCH_SIZE = 50  # Process in batches to manage memory\n",
        "RATE_LIMIT_DELAY = 0.1  # Delay between API calls (seconds)\n",
        "\n",
        "print(f\"üìä Target symbols: {TARGET_SYMBOLS}\")\n",
        "print(f\"‚öôÔ∏è  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"‚è±Ô∏è  Rate limit delay: {RATE_LIMIT_DELAY}s\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Historical Data Collection Functions\n",
        "Advanced functions for large-scale market data collection with error handling and progress monitoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Testing historical data collection functions...\n",
            "üìÖ Date range: 2024-05-24 to 2025-06-28\n",
            "\n",
            "üß™ Testing with INTC sample...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-28 19:35:38,406 - INFO - ‚úÖ Retrieved 273 days of data for INTC\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Sample data collected: 273 trading days\n",
            "üìÖ Date range: 2024-05-24 to 2025-06-27\n",
            "üìä Technical indicators calculated successfully\n",
            "üî¢ Columns: 21 (OHLCV + 16 indicators)\n",
            "\n",
            "üìà Sample data (latest 3 days):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>sma_1</th>\n",
              "      <th>sma_5</th>\n",
              "      <th>roc_1</th>\n",
              "      <th>roc_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2025-06-25</th>\n",
              "      <td>22.20</td>\n",
              "      <td>22.20</td>\n",
              "      <td>21.702</td>\n",
              "      <td>-1.5521</td>\n",
              "      <td>6.7308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-06-26</th>\n",
              "      <td>22.50</td>\n",
              "      <td>22.50</td>\n",
              "      <td>21.904</td>\n",
              "      <td>1.3513</td>\n",
              "      <td>4.6999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-06-27</th>\n",
              "      <td>22.69</td>\n",
              "      <td>22.69</td>\n",
              "      <td>22.226</td>\n",
              "      <td>0.8444</td>\n",
              "      <td>7.6376</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Close  sma_1   sma_5   roc_1   roc_5\n",
              "2025-06-25  22.20  22.20  21.702 -1.5521  6.7308\n",
              "2025-06-26  22.50  22.50  21.904  1.3513  4.6999\n",
              "2025-06-27  22.69  22.69  22.226  0.8444  7.6376"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def fetch_historical_data(symbol, start_date, end_date, retries=3):\n",
        "    \"\"\"\n",
        "    Fetch historical market data with error handling and retries.\n",
        "    \n",
        "    Args:\n",
        "        symbol (str): Stock symbol (e.g., 'INTC')\n",
        "        start_date (str): Start date in 'YYYY-MM-DD' format\n",
        "        end_date (str): End date in 'YYYY-MM-DD' format\n",
        "        retries (int): Number of retry attempts\n",
        "    \n",
        "    Returns:\n",
        "        pandas.DataFrame: Historical OHLCV data or None if failed\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Rate limiting\n",
        "            time.sleep(RATE_LIMIT_DELAY)\n",
        "            \n",
        "            # Fetch data\n",
        "            ticker = yf.Ticker(symbol)\n",
        "            data = ticker.history(start=start_date, end=end_date)\n",
        "            \n",
        "            if data.empty:\n",
        "                logger.warning(f\"No data retrieved for {symbol} ({start_date} to {end_date})\")\n",
        "                return None\n",
        "                \n",
        "            # Clean the data\n",
        "            data = data.drop(columns=['Dividends', 'Stock Splits'], errors='ignore')\n",
        "            data.index = data.index.date  # Convert to date only\n",
        "            \n",
        "            logger.info(f\"‚úÖ Retrieved {len(data)} days of data for {symbol}\")\n",
        "            return data\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Attempt {attempt + 1} failed for {symbol}: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "            else:\n",
        "                logger.error(f\"‚ùå Failed to fetch data for {symbol} after {retries} attempts\")\n",
        "                return None\n",
        "\n",
        "def calculate_technical_indicators_batch(data):\n",
        "    \"\"\"\n",
        "    Calculate technical indicators with optimized batch processing.\n",
        "    \n",
        "    Args:\n",
        "        data (pandas.DataFrame): OHLCV data\n",
        "    \n",
        "    Returns:\n",
        "        pandas.DataFrame: Data with technical indicators\n",
        "    \"\"\"\n",
        "    if data is None or data.empty:\n",
        "        return None\n",
        "        \n",
        "    result = data.copy()\n",
        "    \n",
        "    try:\n",
        "        # Vectorized calculations for performance\n",
        "        close_prices = result['Close']\n",
        "        \n",
        "        # Simple Moving Averages (1-8 days) - vectorized\n",
        "        for period in range(1, 9):\n",
        "            result[f'sma_{period}'] = close_prices.rolling(window=period, min_periods=1).mean()\n",
        "        \n",
        "        # Rate of Change (1-8 days) - vectorized\n",
        "        for period in range(1, 9):\n",
        "            result[f'roc_{period}'] = close_prices.pct_change(periods=period) * 100\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error calculating technical indicators: {e}\")\n",
        "        return data\n",
        "\n",
        "def get_optimal_date_range():\n",
        "    \"\"\"\n",
        "    Calculate optimal date range for historical data collection.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (start_date, end_date) as strings\n",
        "    \"\"\"\n",
        "    # Get 1 year + buffer for weekends/holidays\n",
        "    end_date = date.today()\n",
        "    start_date = end_date - timedelta(days=400)  # ~1.1 years to ensure we get 252+ trading days\n",
        "    \n",
        "    return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')\n",
        "\n",
        "# Test the functions\n",
        "print(\"üîç Testing historical data collection functions...\")\n",
        "\n",
        "# Get optimal date range\n",
        "start_date, end_date = get_optimal_date_range()\n",
        "print(f\"üìÖ Date range: {start_date} to {end_date}\")\n",
        "\n",
        "# Test with a small sample\n",
        "print(\"\\nüß™ Testing with INTC sample...\")\n",
        "test_sample = fetch_historical_data(\"INTC\", start_date, end_date)\n",
        "\n",
        "if test_sample is not None:\n",
        "    print(f\"‚úÖ Sample data collected: {len(test_sample)} trading days\")\n",
        "    print(f\"üìÖ Date range: {test_sample.index[0]} to {test_sample.index[-1]}\")\n",
        "    \n",
        "    # Test technical indicators\n",
        "    test_with_indicators = calculate_technical_indicators_batch(test_sample)\n",
        "    if test_with_indicators is not None:\n",
        "        print(f\"üìä Technical indicators calculated successfully\")\n",
        "        print(f\"üî¢ Columns: {len(test_with_indicators.columns)} (OHLCV + 16 indicators)\")\n",
        "        \n",
        "        # Show sample\n",
        "        print(\"\\nüìà Sample data (latest 3 days):\")\n",
        "        sample_cols = ['Close', 'sma_1', 'sma_5', 'roc_1', 'roc_5']\n",
        "        display(test_with_indicators[sample_cols].tail(3).round(4))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Sample test failed - will continue with error handling\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Batch Database Storage Functions\n",
        "Optimized database operations for large-scale historical data processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-28 19:35:51,300 - INFO - Processing 5 records for INTC (ID: 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Testing batch storage with sample data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d48f4ed91b1c46e49e64ef30cdbf48ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Storing INTC data:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-28 19:35:51,544 - INFO - ‚úÖ Completed INTC: 5 records processed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Batch storage test successful!\n",
            "üìä Records processed: 5\n",
            "üíæ Market data: 5\n",
            "üìà Indicators: 5\n"
          ]
        }
      ],
      "source": [
        "def store_historical_data_batch(symbol, data_with_indicators, batch_size=BATCH_SIZE):\n",
        "    \"\"\"\n",
        "    Store historical data and technical indicators in batches for optimal performance.\n",
        "    \n",
        "    Args:\n",
        "        symbol (str): Stock symbol\n",
        "        data_with_indicators (pandas.DataFrame): Historical data with technical indicators\n",
        "        batch_size (int): Number of records to process per batch\n",
        "    \n",
        "    Returns:\n",
        "        dict: Processing results with counts and status\n",
        "    \"\"\"\n",
        "    if data_with_indicators is None or data_with_indicators.empty:\n",
        "        logger.warning(\"No data to store\")\n",
        "        return {'success': False, 'records_processed': 0, 'errors': ['No data']}\n",
        "        \n",
        "    try:\n",
        "        engine = get_database_connection()\n",
        "        results = {\n",
        "            'success': True,\n",
        "            'records_processed': 0,\n",
        "            'market_data_inserted': 0,\n",
        "            'indicators_inserted': 0,\n",
        "            'errors': []\n",
        "        }\n",
        "        \n",
        "        # Get symbol_id once\n",
        "        with engine.connect() as conn:\n",
        "            symbol_query = \"SELECT id FROM symbols WHERE symbol = :symbol\"\n",
        "            symbol_result = conn.execute(sqlalchemy.text(symbol_query), {'symbol': symbol})\n",
        "            symbol_row = symbol_result.fetchone()\n",
        "            \n",
        "            if not symbol_row:\n",
        "                error_msg = f\"Symbol {symbol} not found in database\"\n",
        "                logger.error(error_msg)\n",
        "                return {'success': False, 'records_processed': 0, 'errors': [error_msg]}\n",
        "                \n",
        "            symbol_id = symbol_row[0]\n",
        "            logger.info(f\"Processing {len(data_with_indicators)} records for {symbol} (ID: {symbol_id})\")\n",
        "        \n",
        "        # Process data in batches\n",
        "        total_records = len(data_with_indicators)\n",
        "        \n",
        "        with tqdm(total=total_records, desc=f\"Storing {symbol} data\") as pbar:\n",
        "            for start_idx in range(0, total_records, batch_size):\n",
        "                end_idx = min(start_idx + batch_size, total_records)\n",
        "                batch_data = data_with_indicators.iloc[start_idx:end_idx]\n",
        "                \n",
        "                try:\n",
        "                    # Process batch with transaction\n",
        "                    with engine.begin() as trans:\n",
        "                        batch_market, batch_indicators = process_data_batch(\n",
        "                            trans, symbol_id, batch_data\n",
        "                        )\n",
        "                        \n",
        "                        results['market_data_inserted'] += batch_market\n",
        "                        results['indicators_inserted'] += batch_indicators\n",
        "                        results['records_processed'] += len(batch_data)\n",
        "                        \n",
        "                    pbar.update(len(batch_data))\n",
        "                    \n",
        "                    # Brief pause between batches\n",
        "                    time.sleep(0.01)\n",
        "                    \n",
        "                except Exception as batch_error:\n",
        "                    error_msg = f\"Batch error ({start_idx}-{end_idx}): {batch_error}\"\n",
        "                    logger.error(error_msg)\n",
        "                    results['errors'].append(error_msg)\n",
        "                    pbar.update(len(batch_data))\n",
        "                    continue\n",
        "        \n",
        "        logger.info(f\"‚úÖ Completed {symbol}: {results['records_processed']} records processed\")\n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = f\"Critical error storing data for {symbol}: {e}\"\n",
        "        logger.error(error_msg)\n",
        "        return {'success': False, 'records_processed': 0, 'errors': [error_msg]}\n",
        "\n",
        "def process_data_batch(transaction, symbol_id, batch_data):\n",
        "    \"\"\"\n",
        "    Process a single batch of data within a database transaction.\n",
        "    \n",
        "    Args:\n",
        "        transaction: Database transaction object\n",
        "        symbol_id (int): Symbol ID from database\n",
        "        batch_data (pandas.DataFrame): Batch of data to process\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (market_records_inserted, indicator_records_inserted)\n",
        "    \"\"\"\n",
        "    market_records = 0\n",
        "    indicator_records = 0\n",
        "    \n",
        "    # SQL queries as variables to avoid linter issues\n",
        "    market_query = (\n",
        "        \"INSERT INTO market_data (symbol_id, trade_date, open_price, high_price, low_price, close_price, volume) \"\n",
        "        \"VALUES (:symbol_id, :trade_date, :open_price, :high_price, :low_price, :close_price, :volume) \"\n",
        "        \"ON CONFLICT (symbol_id, trade_date) DO UPDATE SET \"\n",
        "        \"open_price = EXCLUDED.open_price, high_price = EXCLUDED.high_price, \"\n",
        "        \"low_price = EXCLUDED.low_price, close_price = EXCLUDED.close_price, \"\n",
        "        \"volume = EXCLUDED.volume, updated_at = CURRENT_TIMESTAMP\"\n",
        "    )\n",
        "    \n",
        "    indicators_query = (\n",
        "        \"INSERT INTO technical_indicators \"\n",
        "        \"(symbol_id, trade_date, sma_1, sma_2, sma_3, sma_4, sma_5, sma_6, sma_7, sma_8, \"\n",
        "        \"roc_1, roc_2, roc_3, roc_4, roc_5, roc_6, roc_7, roc_8) \"\n",
        "        \"VALUES (:symbol_id, :trade_date, :sma_1, :sma_2, :sma_3, :sma_4, :sma_5, :sma_6, :sma_7, :sma_8, \"\n",
        "        \":roc_1, :roc_2, :roc_3, :roc_4, :roc_5, :roc_6, :roc_7, :roc_8) \"\n",
        "        \"ON CONFLICT (symbol_id, trade_date) DO UPDATE SET \"\n",
        "        \"sma_1 = EXCLUDED.sma_1, sma_2 = EXCLUDED.sma_2, sma_3 = EXCLUDED.sma_3, sma_4 = EXCLUDED.sma_4, \"\n",
        "        \"sma_5 = EXCLUDED.sma_5, sma_6 = EXCLUDED.sma_6, sma_7 = EXCLUDED.sma_7, sma_8 = EXCLUDED.sma_8, \"\n",
        "        \"roc_1 = EXCLUDED.roc_1, roc_2 = EXCLUDED.roc_2, roc_3 = EXCLUDED.roc_3, roc_4 = EXCLUDED.roc_4, \"\n",
        "        \"roc_5 = EXCLUDED.roc_5, roc_6 = EXCLUDED.roc_6, roc_7 = EXCLUDED.roc_7, roc_8 = EXCLUDED.roc_8, \"\n",
        "        \"updated_at = CURRENT_TIMESTAMP\"\n",
        "    )\n",
        "    \n",
        "    for trade_date, row in batch_data.iterrows():\n",
        "        # Insert market data\n",
        "        transaction.execute(sqlalchemy.text(market_query), {\n",
        "            'symbol_id': symbol_id,\n",
        "            'trade_date': trade_date,\n",
        "            'open_price': float(row['Open']),\n",
        "            'high_price': float(row['High']),\n",
        "            'low_price': float(row['Low']),\n",
        "            'close_price': float(row['Close']),\n",
        "            'volume': int(row['Volume'])\n",
        "        })\n",
        "        market_records += 1\n",
        "        \n",
        "        # Prepare technical indicators data\n",
        "        indicators_data = {'symbol_id': symbol_id, 'trade_date': trade_date}\n",
        "        for i in range(1, 9):\n",
        "            indicators_data[f'sma_{i}'] = float(row[f'sma_{i}']) if not pd.isna(row[f'sma_{i}']) else None\n",
        "            indicators_data[f'roc_{i}'] = float(row[f'roc_{i}']) if not pd.isna(row[f'roc_{i}']) else None\n",
        "        \n",
        "        transaction.execute(sqlalchemy.text(indicators_query), indicators_data)\n",
        "        indicator_records += 1\n",
        "    \n",
        "    return market_records, indicator_records\n",
        "\n",
        "# Test batch storage with sample data\n",
        "if 'test_with_indicators' in locals() and test_with_indicators is not None:\n",
        "    print(\"\\nüîç Testing batch storage with sample data...\")\n",
        "    \n",
        "    # Test with just the last 5 records\n",
        "    test_batch = test_with_indicators.tail(5)\n",
        "    storage_results = store_historical_data_batch(\"INTC\", test_batch)\n",
        "    \n",
        "    if storage_results['success']:\n",
        "        print(f\"‚úÖ Batch storage test successful!\")\n",
        "        print(f\"üìä Records processed: {storage_results['records_processed']}\")\n",
        "        print(f\"üíæ Market data: {storage_results['market_data_inserted']}\")\n",
        "        print(f\"üìà Indicators: {storage_results['indicators_inserted']}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Batch storage test failed: {storage_results['errors']}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No test data available for batch storage test\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Multi-Symbol Processing Orchestrator\n",
        "Master function to coordinate historical data backfill across all symbols with comprehensive progress monitoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Historical backfill orchestrator ready!\n",
            "üìä Functions available:\n",
            "  ‚Ä¢ run_historical_backfill() - Execute complete backfill\n",
            "  ‚Ä¢ validate_historical_data() - Validate stored data\n",
            "\n",
            "üí° Ready to process 1+ year of historical data for all symbols!\n"
          ]
        }
      ],
      "source": [
        "def run_historical_backfill(symbols=TARGET_SYMBOLS):\n",
        "    \"\"\"\n",
        "    Master orchestrator for historical data backfill across multiple symbols.\n",
        "    \n",
        "    Args:\n",
        "        symbols (list): List of stock symbols to process\n",
        "    \n",
        "    Returns:\n",
        "        dict: Comprehensive processing results and statistics\n",
        "    \"\"\"\n",
        "    logger.info(\"üöÄ Starting Historical Data Backfill - Phase 1C\")\n",
        "    logger.info(f\"üìä Processing symbols: {symbols}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    start_date, end_date = get_optimal_date_range()\n",
        "    \n",
        "    # Initialize results tracking\n",
        "    master_results = {\n",
        "        'start_time': datetime.now(),\n",
        "        'symbols_processed': [],\n",
        "        'total_trading_days': 0,\n",
        "        'total_market_records': 0,\n",
        "        'total_indicator_records': 0,\n",
        "        'processing_times': {},\n",
        "        'errors': [],\n",
        "        'success_rate': 0.0\n",
        "    }\n",
        "    \n",
        "    logger.info(f\"üìÖ Date range: {start_date} to {end_date}\")\n",
        "    \n",
        "    # Process each symbol\n",
        "    with tqdm(total=len(symbols), desc=\"Processing symbols\", position=0) as symbol_pbar:\n",
        "        for symbol in symbols:\n",
        "            symbol_start_time = time.time()\n",
        "            logger.info(f\"\\nüìà Processing {symbol}...\")\n",
        "            \n",
        "            try:\n",
        "                # Step 1: Fetch historical data\n",
        "                logger.info(f\"üì• Fetching historical data for {symbol}...\")\n",
        "                raw_data = fetch_historical_data(symbol, start_date, end_date)\n",
        "                \n",
        "                if raw_data is None:\n",
        "                    error_msg = f\"Failed to fetch data for {symbol}\"\n",
        "                    logger.error(error_msg)\n",
        "                    master_results['errors'].append(error_msg)\n",
        "                    symbol_pbar.update(1)\n",
        "                    continue\n",
        "                \n",
        "                # Step 2: Calculate technical indicators\n",
        "                logger.info(f\"üìä Calculating technical indicators for {symbol}...\")\n",
        "                data_with_indicators = calculate_technical_indicators_batch(raw_data)\n",
        "                \n",
        "                if data_with_indicators is None:\n",
        "                    error_msg = f\"Failed to calculate indicators for {symbol}\"\n",
        "                    logger.error(error_msg)\n",
        "                    master_results['errors'].append(error_msg)\n",
        "                    symbol_pbar.update(1)\n",
        "                    continue\n",
        "                \n",
        "                # Step 3: Store in database\n",
        "                logger.info(f\"üíæ Storing {len(data_with_indicators)} records for {symbol}...\")\n",
        "                storage_results = store_historical_data_batch(symbol, data_with_indicators)\n",
        "                \n",
        "                if storage_results['success']:\n",
        "                    # Update master results\n",
        "                    symbol_processing_time = time.time() - symbol_start_time\n",
        "                    master_results['symbols_processed'].append(symbol)\n",
        "                    master_results['total_trading_days'] += storage_results['records_processed']\n",
        "                    master_results['total_market_records'] += storage_results['market_data_inserted']\n",
        "                    master_results['total_indicator_records'] += storage_results['indicators_inserted']\n",
        "                    master_results['processing_times'][symbol] = symbol_processing_time\n",
        "                    \n",
        "                    logger.info(f\"‚úÖ {symbol} completed successfully!\")\n",
        "                    logger.info(f\"üìä Records: {storage_results['records_processed']}\")\n",
        "                    logger.info(f\"‚è±Ô∏è  Processing time: {symbol_processing_time:.2f}s\")\n",
        "                else:\n",
        "                    error_msg = f\"Failed to store data for {symbol}: {storage_results['errors']}\"\n",
        "                    logger.error(error_msg)\n",
        "                    master_results['errors'].extend(storage_results['errors'])\n",
        "                \n",
        "                symbol_pbar.update(1)\n",
        "                \n",
        "                # Brief pause between symbols\n",
        "                time.sleep(0.5)\n",
        "                \n",
        "            except Exception as e:\n",
        "                error_msg = f\"Critical error processing {symbol}: {e}\"\n",
        "                logger.error(error_msg)\n",
        "                master_results['errors'].append(error_msg)\n",
        "                symbol_pbar.update(1)\n",
        "                continue\n",
        "    \n",
        "    # Calculate final statistics\n",
        "    total_time = time.time() - start_time\n",
        "    master_results['end_time'] = datetime.now()\n",
        "    master_results['total_processing_time'] = total_time\n",
        "    master_results['success_rate'] = len(master_results['symbols_processed']) / len(symbols) * 100\n",
        "    \n",
        "    # Final report\n",
        "    logger.info(\"\\n\" + \"=\"*60)\n",
        "    logger.info(\"üéØ HISTORICAL DATA BACKFILL COMPLETED!\")\n",
        "    logger.info(\"=\"*60)\n",
        "    logger.info(f\"‚úÖ Symbols processed: {len(master_results['symbols_processed'])}/{len(symbols)}\")\n",
        "    logger.info(f\"üìä Success rate: {master_results['success_rate']:.1f}%\")\n",
        "    logger.info(f\"üìà Total trading days: {master_results['total_trading_days']}\")\n",
        "    logger.info(f\"üíæ Market records: {master_results['total_market_records']}\")\n",
        "    logger.info(f\"üìä Indicator records: {master_results['total_indicator_records']}\")\n",
        "    logger.info(f\"‚è±Ô∏è  Total time: {total_time:.2f}s\")\n",
        "    \n",
        "    if master_results['errors']:\n",
        "        logger.warning(f\"‚ö†Ô∏è  Errors encountered: {len(master_results['errors'])}\")\n",
        "    \n",
        "    return master_results\n",
        "\n",
        "def validate_historical_data():\n",
        "    \"\"\"\n",
        "    Comprehensive validation of stored historical data.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Validation results and statistics\n",
        "    \"\"\"\n",
        "    logger.info(\"üîç Validating historical data...\")\n",
        "    \n",
        "    try:\n",
        "        engine = get_database_connection()\n",
        "        validation_results = {}\n",
        "        \n",
        "        with engine.connect() as conn:\n",
        "            # Check market data coverage\n",
        "            market_coverage_query = (\n",
        "                \"SELECT s.symbol, \"\n",
        "                \"COUNT(md.id) as trading_days, \"\n",
        "                \"MIN(md.trade_date) as earliest_date, \"\n",
        "                \"MAX(md.trade_date) as latest_date, \"\n",
        "                \"AVG(md.volume) as avg_volume \"\n",
        "                \"FROM symbols s \"\n",
        "                \"LEFT JOIN market_data md ON s.id = md.symbol_id \"\n",
        "                \"WHERE s.symbol IN ('INTC', 'AMD', 'NVDA') \"\n",
        "                \"GROUP BY s.symbol \"\n",
        "                \"ORDER BY s.symbol\"\n",
        "            )\n",
        "            \n",
        "            market_df = pd.read_sql(market_coverage_query, conn)\n",
        "            validation_results['market_coverage'] = market_df.to_dict('records')\n",
        "            \n",
        "            # Check technical indicators coverage\n",
        "            indicators_coverage_query = (\n",
        "                \"SELECT s.symbol, \"\n",
        "                \"COUNT(ti.id) as indicator_records, \"\n",
        "                \"COUNT(CASE WHEN ti.sma_5 IS NOT NULL THEN 1 END) as sma_5_count, \"\n",
        "                \"COUNT(CASE WHEN ti.roc_5 IS NOT NULL THEN 1 END) as roc_5_count \"\n",
        "                \"FROM symbols s \"\n",
        "                \"LEFT JOIN technical_indicators ti ON s.id = ti.symbol_id \"\n",
        "                \"WHERE s.symbol IN ('INTC', 'AMD', 'NVDA') \"\n",
        "                \"GROUP BY s.symbol \"\n",
        "                \"ORDER BY s.symbol\"\n",
        "            )\n",
        "            \n",
        "            indicators_df = pd.read_sql(indicators_coverage_query, conn)\n",
        "            validation_results['indicators_coverage'] = indicators_df.to_dict('records')\n",
        "            \n",
        "            # Data quality checks\n",
        "            quality_query = (\n",
        "                \"SELECT s.symbol, \"\n",
        "                \"COUNT(CASE WHEN md.volume = 0 THEN 1 END) as zero_volume_days, \"\n",
        "                \"COUNT(CASE WHEN md.high_price < md.low_price THEN 1 END) as price_anomalies \"\n",
        "                \"FROM symbols s \"\n",
        "                \"JOIN market_data md ON s.id = md.symbol_id \"\n",
        "                \"WHERE s.symbol IN ('INTC', 'AMD', 'NVDA') \"\n",
        "                \"GROUP BY s.symbol \"\n",
        "                \"ORDER BY s.symbol\"\n",
        "            )\n",
        "            \n",
        "            quality_df = pd.read_sql(quality_query, conn)\n",
        "            validation_results['data_quality'] = quality_df.to_dict('records')\n",
        "        \n",
        "        logger.info(\"‚úÖ Data validation completed successfully\")\n",
        "        return validation_results\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Validation failed: {e}\")\n",
        "        return {'error': str(e)}\n",
        "\n",
        "print(\"üöÄ Historical backfill orchestrator ready!\")\n",
        "print(\"üìä Functions available:\")\n",
        "print(\"  ‚Ä¢ run_historical_backfill() - Execute complete backfill\")\n",
        "print(\"  ‚Ä¢ validate_historical_data() - Validate stored data\")\n",
        "print(\"\\nüí° Ready to process 1+ year of historical data for all symbols!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Execute Historical Data Backfill\n",
        "**üöÄ PHASE 1C EXECUTION - Ready to process 1+ year of historical data!**\n",
        "\n",
        "Execute this section to run the complete historical data backfill for all target symbols.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-28 19:36:26,808 - INFO - üöÄ Starting Historical Data Backfill - Phase 1C\n",
            "2025-06-28 19:36:26,809 - INFO - üìä Processing symbols: ['INTC', 'AMD', 'NVDA']\n",
            "2025-06-28 19:36:26,809 - INFO - üìÖ Date range: 2024-05-24 to 2025-06-28\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üéØ PHASE 1C: HISTORICAL DATA BACKFILL\n",
            "============================================================\n",
            "üìä Target Symbols: ['INTC', 'AMD', 'NVDA']\n",
            "üìÖ Expected Time Range: ~1 year (252+ trading days)\n",
            "üíæ Expected Records: ~750+ market records + 750+ indicator records\n",
            "\n",
            "‚ö†Ô∏è  This will process large amounts of data and may take several minutes.\n",
            "üí° Progress will be shown with detailed logging and progress bars.\n",
            "\n",
            "üöÄ Starting historical data backfill...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "596947835044482282977c2f6116fc2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing symbols:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-28 19:36:26,813 - INFO - \n",
            "üìà Processing INTC...\n",
            "2025-06-28 19:36:26,813 - INFO - üì• Fetching historical data for INTC...\n",
            "2025-06-28 19:36:26,925 - INFO - ‚úÖ Retrieved 273 days of data for INTC\n",
            "2025-06-28 19:36:26,926 - INFO - üìä Calculating technical indicators for INTC...\n",
            "2025-06-28 19:36:26,930 - INFO - üíæ Storing 273 records for INTC...\n",
            "2025-06-28 19:36:26,945 - INFO - Processing 273 records for INTC (ID: 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14a6d1fab15345f3a4c29118345dc7d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Storing INTC data:   0%|          | 0/273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-28 19:36:27,194 - INFO - ‚úÖ Completed INTC: 273 records processed\n",
            "2025-06-28 19:36:27,195 - INFO - ‚úÖ INTC completed successfully!\n",
            "2025-06-28 19:36:27,195 - INFO - üìä Records: 273\n",
            "2025-06-28 19:36:27,195 - INFO - ‚è±Ô∏è  Processing time: 0.38s\n",
            "2025-06-28 19:36:27,700 - INFO - \n",
            "üìà Processing AMD...\n",
            "2025-06-28 19:36:27,701 - INFO - üì• Fetching historical data for AMD...\n",
            "2025-06-28 19:36:27,911 - INFO - ‚úÖ Retrieved 273 days of data for AMD\n",
            "2025-06-28 19:36:27,912 - INFO - üìä Calculating technical indicators for AMD...\n",
            "2025-06-28 19:36:27,916 - INFO - üíæ Storing 273 records for AMD...\n",
            "2025-06-28 19:36:27,932 - INFO - Processing 273 records for AMD (ID: 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d625d278164945598332d377df45e5f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Storing AMD data:   0%|          | 0/273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-28 19:36:28,185 - INFO - ‚úÖ Completed AMD: 273 records processed\n",
            "2025-06-28 19:36:28,185 - INFO - ‚úÖ AMD completed successfully!\n",
            "2025-06-28 19:36:28,186 - INFO - üìä Records: 273\n",
            "2025-06-28 19:36:28,186 - INFO - ‚è±Ô∏è  Processing time: 0.48s\n",
            "2025-06-28 19:36:28,688 - INFO - \n",
            "üìà Processing NVDA...\n",
            "2025-06-28 19:36:28,689 - INFO - üì• Fetching historical data for NVDA...\n",
            "2025-06-28 19:36:28,941 - INFO - ‚úÖ Retrieved 273 days of data for NVDA\n",
            "2025-06-28 19:36:28,941 - INFO - üìä Calculating technical indicators for NVDA...\n",
            "2025-06-28 19:36:28,946 - INFO - üíæ Storing 273 records for NVDA...\n",
            "2025-06-28 19:36:28,958 - INFO - Processing 273 records for NVDA (ID: 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e77a51ea60f421a80cfc6e956ba6c5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Storing NVDA data:   0%|          | 0/273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-28 19:36:29,205 - INFO - ‚úÖ Completed NVDA: 273 records processed\n",
            "2025-06-28 19:36:29,206 - INFO - ‚úÖ NVDA completed successfully!\n",
            "2025-06-28 19:36:29,206 - INFO - üìä Records: 273\n",
            "2025-06-28 19:36:29,207 - INFO - ‚è±Ô∏è  Processing time: 0.52s\n",
            "2025-06-28 19:36:29,708 - INFO - \n",
            "============================================================\n",
            "2025-06-28 19:36:29,710 - INFO - üéØ HISTORICAL DATA BACKFILL COMPLETED!\n",
            "2025-06-28 19:36:29,711 - INFO - ============================================================\n",
            "2025-06-28 19:36:29,712 - INFO - ‚úÖ Symbols processed: 3/3\n",
            "2025-06-28 19:36:29,715 - INFO - üìä Success rate: 100.0%\n",
            "2025-06-28 19:36:29,716 - INFO - üìà Total trading days: 819\n",
            "2025-06-28 19:36:29,717 - INFO - üíæ Market records: 819\n",
            "2025-06-28 19:36:29,718 - INFO - üìä Indicator records: 819\n",
            "2025-06-28 19:36:29,718 - INFO - ‚è±Ô∏è  Total time: 2.90s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üìä BACKFILL RESULTS SUMMARY\n",
            "============================================================\n",
            "‚úÖ Success Rate: 100.0%\n",
            "üìà Symbols Processed: 3\n",
            "üìä Total Trading Days: 819\n",
            "üíæ Market Records: 819\n",
            "üìä Indicator Records: 819\n",
            "‚è±Ô∏è  Total Processing Time: 2.90s\n",
            "‚ö° Average Time per Symbol: 0.46s\n",
            "\n",
            "üìä Per-Symbol Performance:\n",
            "  ‚Ä¢ INTC: 0.38s (~273 records)\n",
            "  ‚Ä¢ AMD: 0.48s (~273 records)\n",
            "  ‚Ä¢ NVDA: 0.52s (~273 records)\n",
            "\n",
            "üéâ Phase 1C Historical Data Backfill Complete!\n",
            "Next: Run validation to verify data quality and completeness.\n"
          ]
        }
      ],
      "source": [
        "# üöÄ EXECUTE HISTORICAL DATA BACKFILL\n",
        "print(\"=\"*60)\n",
        "print(\"üéØ PHASE 1C: HISTORICAL DATA BACKFILL\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üìä Target Symbols: {TARGET_SYMBOLS}\")\n",
        "print(f\"üìÖ Expected Time Range: ~1 year (252+ trading days)\")\n",
        "print(f\"üíæ Expected Records: ~750+ market records + 750+ indicator records\")\n",
        "print()\n",
        "\n",
        "# Confirm execution\n",
        "print(\"‚ö†Ô∏è  This will process large amounts of data and may take several minutes.\")\n",
        "print(\"üí° Progress will be shown with detailed logging and progress bars.\")\n",
        "print()\n",
        "\n",
        "# Execute the backfill\n",
        "print(\"üöÄ Starting historical data backfill...\")\n",
        "backfill_results = run_historical_backfill()\n",
        "\n",
        "# Display results summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä BACKFILL RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if backfill_results['success_rate'] > 0:\n",
        "    print(f\"‚úÖ Success Rate: {backfill_results['success_rate']:.1f}%\")\n",
        "    print(f\"üìà Symbols Processed: {len(backfill_results['symbols_processed'])}\")\n",
        "    print(f\"üìä Total Trading Days: {backfill_results['total_trading_days']}\")\n",
        "    print(f\"üíæ Market Records: {backfill_results['total_market_records']}\")\n",
        "    print(f\"üìä Indicator Records: {backfill_results['total_indicator_records']}\")\n",
        "    print(f\"‚è±Ô∏è  Total Processing Time: {backfill_results['total_processing_time']:.2f}s\")\n",
        "    \n",
        "    # Performance metrics\n",
        "    if backfill_results['processing_times']:\n",
        "        avg_time = sum(backfill_results['processing_times'].values()) / len(backfill_results['processing_times'])\n",
        "        print(f\"‚ö° Average Time per Symbol: {avg_time:.2f}s\")\n",
        "        \n",
        "        print(\"\\nüìä Per-Symbol Performance:\")\n",
        "        for symbol, proc_time in backfill_results['processing_times'].items():\n",
        "            records_per_symbol = backfill_results['total_trading_days'] // len(backfill_results['symbols_processed'])\n",
        "            print(f\"  ‚Ä¢ {symbol}: {proc_time:.2f}s (~{records_per_symbol} records)\")\n",
        "else:\n",
        "    print(\"‚ùå Backfill failed completely\")\n",
        "\n",
        "if backfill_results['errors']:\n",
        "    print(f\"\\n‚ö†Ô∏è  Errors Encountered: {len(backfill_results['errors'])}\")\n",
        "    for error in backfill_results['errors'][:5]:  # Show first 5 errors\n",
        "        print(f\"  ‚Ä¢ {error}\")\n",
        "    if len(backfill_results['errors']) > 5:\n",
        "        print(f\"  ... and {len(backfill_results['errors']) - 5} more errors\")\n",
        "\n",
        "print(\"\\nüéâ Phase 1C Historical Data Backfill Complete!\")\n",
        "print(\"Next: Run validation to verify data quality and completeness.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Data Validation and Quality Assurance\n",
        "Comprehensive validation of the historical dataset to ensure data quality and completeness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-28 19:36:48,349 - INFO - üîç Validating historical data...\n",
            "2025-06-28 19:36:48,377 - INFO - ‚úÖ Data validation completed successfully\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üîç DATA VALIDATION & QUALITY ASSURANCE\n",
            "============================================================\n",
            "‚úÖ Data validation completed successfully!\n",
            "\n",
            "üìä MARKET DATA COVERAGE\n",
            "----------------------------------------\n",
            "üìà AMD:\n",
            "  ‚Ä¢ Trading Days: 273\n",
            "  ‚Ä¢ Date Range: 2024-05-24 to 2025-06-27\n",
            "  ‚Ä¢ Avg Volume: 43,071,575\n",
            "\n",
            "üìà INTC:\n",
            "  ‚Ä¢ Trading Days: 273\n",
            "  ‚Ä¢ Date Range: 2024-05-24 to 2025-06-27\n",
            "  ‚Ä¢ Avg Volume: 81,708,799\n",
            "\n",
            "üìà NVDA:\n",
            "  ‚Ä¢ Trading Days: 273\n",
            "  ‚Ä¢ Date Range: 2024-05-24 to 2025-06-27\n",
            "  ‚Ä¢ Avg Volume: 282,966,432\n",
            "\n",
            "üìä TECHNICAL INDICATORS COVERAGE\n",
            "----------------------------------------\n",
            "üìà AMD:\n",
            "  ‚Ä¢ Indicator Records: 273\n",
            "  ‚Ä¢ SMA-5 Coverage: 273/273 (100.0%)\n",
            "  ‚Ä¢ ROC-5 Coverage: 268/273 (98.2%)\n",
            "\n",
            "üìà INTC:\n",
            "  ‚Ä¢ Indicator Records: 273\n",
            "  ‚Ä¢ SMA-5 Coverage: 273/273 (100.0%)\n",
            "  ‚Ä¢ ROC-5 Coverage: 268/273 (98.2%)\n",
            "\n",
            "üìà NVDA:\n",
            "  ‚Ä¢ Indicator Records: 273\n",
            "  ‚Ä¢ SMA-5 Coverage: 273/273 (100.0%)\n",
            "  ‚Ä¢ ROC-5 Coverage: 268/273 (98.2%)\n",
            "\n",
            "üîç DATA QUALITY CHECKS\n",
            "----------------------------------------\n",
            "üìà AMD:\n",
            "  ‚Ä¢ Zero Volume Days: 0\n",
            "  ‚Ä¢ Price Anomalies: 0\n",
            "\n",
            "üìà INTC:\n",
            "  ‚Ä¢ Zero Volume Days: 0\n",
            "  ‚Ä¢ Price Anomalies: 0\n",
            "\n",
            "üìà NVDA:\n",
            "  ‚Ä¢ Zero Volume Days: 0\n",
            "  ‚Ä¢ Price Anomalies: 0\n",
            "\n",
            "‚úÖ No data quality issues detected!\n",
            "\n",
            "============================================================\n",
            "üìã PHASE 1C COMPLETION SUMMARY\n",
            "============================================================\n",
            "‚úÖ Symbols Successfully Processed: 3/3\n",
            "üìä Total Market Data Records: 819\n",
            "üìà Total Technical Indicator Records: 819\n",
            "üéØ Data Quality Score: 100.0%\n",
            "\n",
            "üéâ PHASE 1C COMPLETE - ALL SUCCESS CRITERIA MET!\n",
            "‚úÖ 1+ year of historical data collected\n",
            "‚úÖ All 3 symbols (INTC, AMD, NVDA) processed\n",
            "‚úÖ Technical indicators calculated and stored\n",
            "‚úÖ Data quality validated\n",
            "\n",
            "üöÄ Ready for Phase 1D: Strategy Development & Backtesting!\n",
            "\n",
            "============================================================\n",
            "Phase 1C Historical Data Backfill - COMPLETE\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# üîç VALIDATE HISTORICAL DATA\n",
        "print(\"=\"*60)\n",
        "print(\"üîç DATA VALIDATION & QUALITY ASSURANCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Run comprehensive validation\n",
        "validation_results = validate_historical_data()\n",
        "\n",
        "if 'error' not in validation_results:\n",
        "    print(\"‚úÖ Data validation completed successfully!\\n\")\n",
        "    \n",
        "    # Market Data Coverage Report\n",
        "    print(\"üìä MARKET DATA COVERAGE\")\n",
        "    print(\"-\" * 40)\n",
        "    market_coverage = validation_results['market_coverage']\n",
        "    \n",
        "    for symbol_data in market_coverage:\n",
        "        symbol = symbol_data['symbol']\n",
        "        days = symbol_data['trading_days']\n",
        "        earliest = symbol_data['earliest_date']\n",
        "        latest = symbol_data['latest_date']\n",
        "        avg_vol = symbol_data['avg_volume']\n",
        "        \n",
        "        print(f\"üìà {symbol}:\")\n",
        "        print(f\"  ‚Ä¢ Trading Days: {days}\")\n",
        "        print(f\"  ‚Ä¢ Date Range: {earliest} to {latest}\")\n",
        "        print(f\"  ‚Ä¢ Avg Volume: {avg_vol:,.0f}\" if avg_vol else \"  ‚Ä¢ Avg Volume: N/A\")\n",
        "        print()\n",
        "    \n",
        "    # Technical Indicators Coverage Report\n",
        "    print(\"üìä TECHNICAL INDICATORS COVERAGE\")\n",
        "    print(\"-\" * 40)\n",
        "    indicators_coverage = validation_results['indicators_coverage']\n",
        "    \n",
        "    for symbol_data in indicators_coverage:\n",
        "        symbol = symbol_data['symbol']\n",
        "        indicator_records = symbol_data['indicator_records']\n",
        "        sma_5_count = symbol_data['sma_5_count']\n",
        "        roc_5_count = symbol_data['roc_5_count']\n",
        "        \n",
        "        print(f\"üìà {symbol}:\")\n",
        "        print(f\"  ‚Ä¢ Indicator Records: {indicator_records}\")\n",
        "        print(f\"  ‚Ä¢ SMA-5 Coverage: {sma_5_count}/{indicator_records} ({sma_5_count/indicator_records*100:.1f}%)\" if indicator_records > 0 else \"  ‚Ä¢ SMA-5 Coverage: N/A\")\n",
        "        print(f\"  ‚Ä¢ ROC-5 Coverage: {roc_5_count}/{indicator_records} ({roc_5_count/indicator_records*100:.1f}%)\" if indicator_records > 0 else \"  ‚Ä¢ ROC-5 Coverage: N/A\")\n",
        "        print()\n",
        "    \n",
        "    # Data Quality Report\n",
        "    print(\"üîç DATA QUALITY CHECKS\")\n",
        "    print(\"-\" * 40)\n",
        "    quality_data = validation_results['data_quality']\n",
        "    \n",
        "    total_anomalies = 0\n",
        "    for symbol_data in quality_data:\n",
        "        symbol = symbol_data['symbol']\n",
        "        zero_volume = symbol_data['zero_volume_days']\n",
        "        price_anomalies = symbol_data['price_anomalies']\n",
        "        total_anomalies += zero_volume + price_anomalies\n",
        "        \n",
        "        print(f\"üìà {symbol}:\")\n",
        "        print(f\"  ‚Ä¢ Zero Volume Days: {zero_volume}\")\n",
        "        print(f\"  ‚Ä¢ Price Anomalies: {price_anomalies}\")\n",
        "        print()\n",
        "    \n",
        "    if total_anomalies == 0:\n",
        "        print(\"‚úÖ No data quality issues detected!\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Total anomalies found: {total_anomalies}\")\n",
        "    \n",
        "    # Summary Statistics\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìã PHASE 1C COMPLETION SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    total_market_records = sum(item['trading_days'] for item in market_coverage)\n",
        "    total_indicator_records = sum(item['indicator_records'] for item in indicators_coverage)\n",
        "    symbols_with_data = len([item for item in market_coverage if item['trading_days'] > 0])\n",
        "    \n",
        "    print(f\"‚úÖ Symbols Successfully Processed: {symbols_with_data}/3\")\n",
        "    print(f\"üìä Total Market Data Records: {total_market_records:,}\")\n",
        "    print(f\"üìà Total Technical Indicator Records: {total_indicator_records:,}\")\n",
        "    print(f\"üéØ Data Quality Score: {((total_market_records + total_indicator_records - total_anomalies) / (total_market_records + total_indicator_records) * 100):.1f}%\" if (total_market_records + total_indicator_records) > 0 else \"N/A\")\n",
        "    \n",
        "    # Success criteria check\n",
        "    if total_market_records >= 750 and symbols_with_data == 3:\n",
        "        print(\"\\nüéâ PHASE 1C COMPLETE - ALL SUCCESS CRITERIA MET!\")\n",
        "        print(\"‚úÖ 1+ year of historical data collected\")\n",
        "        print(\"‚úÖ All 3 symbols (INTC, AMD, NVDA) processed\")\n",
        "        print(\"‚úÖ Technical indicators calculated and stored\")\n",
        "        print(\"‚úÖ Data quality validated\")\n",
        "        print(\"\\nüöÄ Ready for Phase 1D: Strategy Development & Backtesting!\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  Some success criteria not fully met:\")\n",
        "        if total_market_records < 750:\n",
        "            print(f\"  ‚Ä¢ Need more historical data ({total_market_records}/750+ records)\")\n",
        "        if symbols_with_data < 3:\n",
        "            print(f\"  ‚Ä¢ Missing data for some symbols ({symbols_with_data}/3 symbols)\")\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå Validation failed: {validation_results['error']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Phase 1C Historical Data Backfill - COMPLETE\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
